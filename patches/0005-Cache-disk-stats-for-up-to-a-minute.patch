From f17a8b2a20870eb7ebd158945c6062d2a292f922 Mon Sep 17 00:00:00 2001
From: Thomas Orozco <thomas@orozco.fr>
Date: Mon, 18 Apr 2016 13:59:46 +0200
Subject: [PATCH] Cache disk stats for up to a minute

Accessing disk stats directly in FsHandler (rather than use MachineInfo)
ensures we don't accidentally end up relying on stale data. However, it
also means we'll query those stats a lot more frequently, and will
frequently duplicate the exact same work (e.g. querying stats for the
volume on which `/var/lib/docker` is mounted).

This adds a cache layer for fs stats (which is refreshed when stats are
queried, and expires stats after a minute), which ensures we don't hit
the FS too often for the exact same information (which might be
expensive to obtain in cases where we need to exec another program).

It also adds jitter to the FS handler so that we don't end up with a
thundering herd problem where all FS handlers end up querying disk stats
at the exact same time because they were all created at the same time,
which would defeat the purpose of the cache.

The performance impact of this change (using
https://github.com/krallin/cadvisor-bench) can be viewed here:

- CPU: http://i.imgur.com/7RSTuUS.png (note the jumps in CPU every one
  minute that were eliminated)
- Memory: http://i.imgur.com/Tw6gM9I.png
---
 container/common/fsHandler.go |   4 +-
 fs/fs.go                      |  79 +++------------
 fs/fs_stats_cache.go          | 172 +++++++++++++++++++++++++++++++++
 fs/fs_stats_cache_test.go     | 216 ++++++++++++++++++++++++++++++++++++++++++
 fs/types.go                   |   5 +
 manager/container.go          |  15 +--
 utils/jitter.go               |  31 ++++++
 7 files changed, 442 insertions(+), 80 deletions(-)
 create mode 100644 fs/fs_stats_cache.go
 create mode 100644 fs/fs_stats_cache_test.go
 create mode 100644 utils/jitter.go

diff --git a/container/common/fsHandler.go b/container/common/fsHandler.go
index 98581f5..44c0fbf 100644
--- a/container/common/fsHandler.go
+++ b/container/common/fsHandler.go
@@ -24,6 +24,7 @@ import (
 	"time"
 
 	"github.com/google/cadvisor/fs"
+	"github.com/google/cadvisor/utils"
 
 	"github.com/golang/glog"
 )
@@ -260,9 +261,8 @@ func (fh *realFsHandler) trackUsage() {
 	for {
 		select {
 		case <-fh.stopChan:
-			// TODO: Does this work without sending anything in?
 			return
-		case <-time.After(fh.period):
+		case <-time.After(utils.Jitter(fh.period, 0.25)):
 			start := time.Now()
 			if err := fh.update(); err != nil {
 				glog.Errorf("failed to collect filesystem stats - %v", err)
diff --git a/fs/fs.go b/fs/fs.go
index ec6c477..81f0934 100644
--- a/fs/fs.go
+++ b/fs/fs.go
@@ -31,11 +31,11 @@ import (
 	"time"
 
 	"github.com/golang/glog"
-	zfs "github.com/mistifyio/go-zfs"
 )
 
 type RealFsInfo struct {
 	partitionCache PartitionCache
+	fsStatsCache   FsStatsCache
 }
 
 type Context struct {
@@ -53,6 +53,7 @@ type DockerContext struct {
 func NewFsInfo(context Context) (FsInfo, error) {
 	fsInfo := &RealFsInfo{
 		partitionCache: NewPartitionCache(context),
+		fsStatsCache:   NewFsStatsCache(),
 	}
 
 	partitions := make([]partition, 0)
@@ -66,6 +67,13 @@ func NewFsInfo(context Context) (FsInfo, error) {
 	return fsInfo, nil
 }
 
+func (self *RealFsInfo) RefreshCache() {
+	err := self.partitionCache.Refresh()
+	if err != nil {
+		glog.Warningf("Failed to refresh partition cache: %s")
+	}
+}
+
 func (self *RealFsInfo) GetDeviceForLabel(label string) (string, error) {
 	return self.partitionCache.DeviceNameForLabel(label)
 }
@@ -89,7 +97,7 @@ func (self *RealFsInfo) GetMountpointForDevice(dev string) (string, error) {
 	return p.mountpoint, nil
 }
 
-func (self *RealFsInfo) getFilteredFsInfo(filter func(device string, partition partition) bool, withIoStats bool) ([]Fs, error) {
+func (self *RealFsInfo) getFilteredFsInfo(filter func(_ string, _ partition) bool, withIoStats bool) ([]Fs, error) {
 	filesystemsOut := make([]Fs, 0)
 
 	err := self.partitionCache.ApplyOverPartitions(func(device string, partition partition) error {
@@ -98,24 +106,14 @@ func (self *RealFsInfo) getFilteredFsInfo(filter func(device string, partition p
 		}
 
 		var (
-			err error
 			fs  Fs
+			err error
 		)
 
-		switch partition.fsType {
-		case DeviceMapper.String():
-			fs.Capacity, fs.Free, fs.Available, err = getDMStats(device, partition.blockSize)
-			fs.Type = DeviceMapper
-		case ZFS.String():
-			fs.Capacity, fs.Free, fs.Available, err = getZfstats(device)
-			fs.Type = ZFS
-		default:
-			fs.Capacity, fs.Free, fs.Available, fs.Inodes, fs.InodesFree, err = getVfsStats(partition.mountpoint)
-			fs.Type = VFS
-		}
+		fs.Type, fs.Capacity, fs.Free, fs.Available, fs.Inodes, fs.InodesFree, err = self.fsStatsCache.FsStats(device, partition)
 
 		if err != nil {
-			// Only log, don't throw an error
+			// Only log, don't return an error, move on to the next FS
 			glog.Errorf("Stat fs for %q failed. Error: %v", device, err)
 			return nil
 		}
@@ -134,6 +132,7 @@ func (self *RealFsInfo) getFilteredFsInfo(filter func(device string, partition p
 		return nil, err
 	}
 
+	// TODO: Use a cache here as well?
 	if withIoStats {
 		diskStatsMap, err := getDiskStatsMap("/proc/diskstats")
 		if err != nil {
@@ -289,53 +288,3 @@ func (self *RealFsInfo) GetDirUsage(dir string, timeout time.Duration) (uint64,
 	}
 	return usageInKb * 1024, nil
 }
-
-func (self *RealFsInfo) RefreshCache() {
-	err := self.partitionCache.Refresh()
-	if err != nil {
-		glog.Warningf("Failed to refresh partition cache: %s")
-	}
-}
-
-func getVfsStats(path string) (total uint64, free uint64, avail uint64, inodes uint64, inodesFree uint64, err error) {
-	var s syscall.Statfs_t
-	if err = syscall.Statfs(path, &s); err != nil {
-		return 0, 0, 0, 0, 0, err
-	}
-	total = uint64(s.Frsize) * s.Blocks
-	free = uint64(s.Frsize) * s.Bfree
-	avail = uint64(s.Frsize) * s.Bavail
-	inodes = uint64(s.Files)
-	inodesFree = uint64(s.Ffree)
-	return total, free, avail, inodes, inodesFree, nil
-}
-
-func getDMStats(poolName string, dataBlkSize uint) (uint64, uint64, uint64, error) {
-	out, err := exec.Command("dmsetup", "status", poolName).Output()
-	if err != nil {
-		return 0, 0, 0, err
-	}
-
-	used, total, err := parseDMStatus(string(out))
-	if err != nil {
-		return 0, 0, 0, err
-	}
-
-	used *= 512 * uint64(dataBlkSize)
-	total *= 512 * uint64(dataBlkSize)
-	free := total - used
-
-	return total, free, free, nil
-}
-
-// getZfstats returns ZFS mount stats using zfsutils
-func getZfstats(poolName string) (uint64, uint64, uint64, error) {
-	dataset, err := zfs.GetDataset(poolName)
-	if err != nil {
-		return 0, 0, 0, err
-	}
-
-	total := dataset.Used + dataset.Avail + dataset.Usedbydataset
-
-	return total, dataset.Avail, dataset.Avail, nil
-}
diff --git a/fs/fs_stats_cache.go b/fs/fs_stats_cache.go
new file mode 100644
index 0000000..a1eb6cc
--- /dev/null
+++ b/fs/fs_stats_cache.go
@@ -0,0 +1,172 @@
+// Copyright 2016 Google Inc. All Rights Reserved.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// +build linux
+
+package fs
+
+import (
+	"fmt"
+	"github.com/golang/glog"
+	zfs "github.com/mistifyio/go-zfs"
+	"os/exec"
+	"sync"
+	"syscall"
+	"time"
+)
+
+type cacheEntry struct {
+	// Cache management
+	createdAt time.Time
+	// Actual cache data
+	Type       FsType
+	Capacity   uint64
+	Free       uint64
+	Available  uint64
+	Inodes     uint64
+	InodesFree uint64
+}
+
+type realFsStatsCache struct {
+	cacheLifetime time.Duration
+	cacheLock     sync.RWMutex
+	cache         map[string]cacheEntry
+	statsHelper   statsHelper
+}
+
+func newFsStatsCache(lifetime time.Duration, helper statsHelper) FsStatsCache {
+	return &realFsStatsCache{
+		cacheLifetime: lifetime,
+		cache:         make(map[string]cacheEntry),
+		statsHelper:   helper,
+	}
+}
+
+func NewFsStatsCache() FsStatsCache {
+	return newFsStatsCache(time.Minute, &realStatsHelper{})
+}
+
+func (self *realFsStatsCache) Clear() {
+	self.cacheLock.Lock()
+	defer self.cacheLock.Unlock()
+	self.cache = make(map[string]cacheEntry)
+}
+
+func makeCacheKey(dev string, part partition) string {
+	return fmt.Sprintf("%s.%s.%s", part.fsType, part.mountpoint, dev)
+}
+
+func unwrapCacheEntry(e cacheEntry) (FsType, uint64, uint64, uint64, uint64, uint64, error) {
+	return e.Type, e.Capacity, e.Free, e.Available, e.Inodes, e.InodesFree, nil
+}
+
+func (self *realFsStatsCache) FsStats(dev string, part partition) (FsType, uint64, uint64, uint64, uint64, uint64, error) {
+	cacheKey := makeCacheKey(dev, part)
+	self.cacheLock.RLock()
+	e, ok := self.cache[cacheKey]
+	self.cacheLock.RUnlock()
+
+	if ok {
+		// We have the data in cache. Return it if it's recent enough
+		if time.Since(e.createdAt) < self.cacheLifetime {
+			glog.V(2).Infof("Consuming stats cache for %q: %+v", cacheKey, e)
+			return unwrapCacheEntry(e)
+		}
+	}
+
+	// Our cache entry is too old, or it doesn't exist. Replace it. Note:
+	// this doesn't do anything to prevent a thundering herd. It's up to
+	// the consumer to do so.
+	glog.V(2).Infof("Refreshing stats cache for %q", cacheKey)
+	e = cacheEntry{}
+
+	var err error
+
+	switch part.fsType {
+	case DeviceMapper.String():
+		e.Capacity, e.Free, e.Available, err = self.statsHelper.GetDmStats(dev, part.blockSize)
+		e.Type = DeviceMapper
+	case ZFS.String():
+		e.Capacity, e.Free, e.Available, err = self.statsHelper.GetZfstats(dev)
+		e.Type = ZFS
+	default:
+		e.Capacity, e.Free, e.Available, e.Inodes, e.InodesFree, err = self.statsHelper.GetVfsStats(part.mountpoint)
+		e.Type = VFS
+	}
+
+	// We failed. Don't update the cache with dead data.
+	if err != nil {
+		return "", 0, 0, 0, 0, 0, err
+	}
+
+	// We succeeded. Update the cache and return the data.
+	e.createdAt = time.Now()
+
+	self.cacheLock.Lock()
+	self.cache[cacheKey] = e
+	self.cacheLock.Unlock()
+
+	return unwrapCacheEntry(e)
+}
+
+type realStatsHelper struct{}
+
+type statsHelper interface {
+	GetVfsStats(path string) (total uint64, free uint64, avail uint64, inodes uint64, inodesFree uint64, err error)
+	GetDmStats(poolName string, dataBlkSize uint) (uint64, uint64, uint64, error)
+	GetZfstats(poolName string) (uint64, uint64, uint64, error)
+}
+
+func (*realStatsHelper) GetVfsStats(path string) (total uint64, free uint64, avail uint64, inodes uint64, inodesFree uint64, err error) {
+	var s syscall.Statfs_t
+	if err = syscall.Statfs(path, &s); err != nil {
+		return 0, 0, 0, 0, 0, err
+	}
+	total = uint64(s.Frsize) * s.Blocks
+	free = uint64(s.Frsize) * s.Bfree
+	avail = uint64(s.Frsize) * s.Bavail
+	inodes = uint64(s.Files)
+	inodesFree = uint64(s.Ffree)
+	return total, free, avail, inodes, inodesFree, nil
+}
+
+func (*realStatsHelper) GetDmStats(poolName string, dataBlkSize uint) (uint64, uint64, uint64, error) {
+	out, err := exec.Command("dmsetup", "status", poolName).Output()
+	if err != nil {
+		return 0, 0, 0, err
+	}
+
+	used, total, err := parseDMStatus(string(out))
+	if err != nil {
+		return 0, 0, 0, err
+	}
+
+	used *= 512 * uint64(dataBlkSize)
+	total *= 512 * uint64(dataBlkSize)
+	free := total - used
+
+	return total, free, free, nil
+}
+
+// getZfstats returns ZFS mount stats using zfsutils
+func (*realStatsHelper) GetZfstats(poolName string) (uint64, uint64, uint64, error) {
+	dataset, err := zfs.GetDataset(poolName)
+	if err != nil {
+		return 0, 0, 0, err
+	}
+
+	total := dataset.Used + dataset.Avail + dataset.Usedbydataset
+
+	return total, dataset.Avail, dataset.Avail, nil
+}
diff --git a/fs/fs_stats_cache_test.go b/fs/fs_stats_cache_test.go
new file mode 100644
index 0000000..64a5237
--- /dev/null
+++ b/fs/fs_stats_cache_test.go
@@ -0,0 +1,216 @@
+// Copyright 2016 Google Inc. All Rights Reserved.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// +build linux
+
+package fs
+
+import (
+	"fmt"
+	"github.com/stretchr/testify/assert"
+	"sync"
+	"sync/atomic"
+	"testing"
+	"time"
+)
+
+type testStatsHelper struct {
+	CacheMissCount uint32
+}
+
+func (self *testStatsHelper) GetVfsStats(path string) (uint64, uint64, uint64, uint64, uint64, error) {
+	atomic.AddUint32(&self.CacheMissCount, 1)
+
+	if path == "/" {
+		return 1000, 400, 500, 100, 20, nil
+	}
+
+	return 0, 0, 0, 0, 0, fmt.Errorf("Unexpected VFS mountpoint: %q", path)
+}
+
+func (self *testStatsHelper) GetZfstats(poolName string) (uint64, uint64, uint64, error) {
+	atomic.AddUint32(&self.CacheMissCount, 1)
+
+	if poolName == "some-pool" {
+		return 2000, 1000, 1500, nil
+	}
+
+	return 0, 0, 0, fmt.Errorf("Unexpected ZFS pool: %q", poolName)
+}
+
+func (self *testStatsHelper) GetDmStats(poolName string, dataBlkSize uint) (uint64, uint64, uint64, error) {
+	atomic.AddUint32(&self.CacheMissCount, 1)
+
+	if poolName == "some-dm" && dataBlkSize == 2048 {
+		return 1500, 100, 200, nil
+	}
+
+	return 0, 0, 0, fmt.Errorf("Unexpected DM pool or block size: %q %d", poolName, dataBlkSize)
+}
+
+var testCases = []struct {
+	device           string
+	fsType           string
+	mountpoint       string
+	blockSize        uint
+	expectFsType     FsType
+	expectCapacity   uint64
+	expectFree       uint64
+	expectAvailable  uint64
+	expectInodes     uint64
+	expectInodesFree uint64
+	expectErr        bool
+}{
+	{
+		device:           "/dev/sda1",
+		fsType:           "ext4",
+		mountpoint:       "/",
+		expectFsType:     VFS,
+		expectCapacity:   1000,
+		expectFree:       400,
+		expectAvailable:  500,
+		expectInodes:     100,
+		expectInodesFree: 20,
+		expectErr:        false,
+	},
+	{
+		device:           "/dev/sda1",
+		fsType:           "ext4",
+		mountpoint:       "/does-not-exist",
+		expectFsType:     "",
+		expectCapacity:   0,
+		expectFree:       0,
+		expectAvailable:  0,
+		expectInodes:     0,
+		expectInodesFree: 0,
+		expectErr:        true,
+	},
+	{
+		device:           "some-pool",
+		fsType:           "zfs",
+		expectFsType:     ZFS,
+		expectCapacity:   2000,
+		expectFree:       1000,
+		expectAvailable:  1500,
+		expectInodes:     0,
+		expectInodesFree: 0,
+		expectErr:        false,
+	},
+	{
+		device:           "some-dm",
+		blockSize:        2048,
+		expectFsType:     DeviceMapper,
+		fsType:           "devicemapper",
+		expectCapacity:   1500,
+		expectFree:       100,
+		expectAvailable:  200,
+		expectInodes:     0,
+		expectInodesFree: 0,
+		expectErr:        false,
+	},
+}
+
+func TestCaching(t *testing.T) {
+	as := assert.New(t)
+
+	for _, testCase := range testCases {
+		helper := testStatsHelper{}
+		fsCache := newFsStatsCache(time.Minute, &helper)
+		as.Equal(uint32(0), helper.CacheMissCount)
+
+		part := partition{
+			fsType:     testCase.fsType,
+			mountpoint: testCase.mountpoint,
+			blockSize:  testCase.blockSize,
+		}
+
+		fsType, capacity, free, available, inodes, inodesFree, err := fsCache.FsStats(testCase.device, part)
+		as.Equal(uint32(1), helper.CacheMissCount)
+
+		if testCase.expectErr {
+			as.Error(err)
+			_, _, _, _, _, _, err = fsCache.FsStats(testCase.device, part)
+			as.Error(err)
+			as.Equal(uint32(2), helper.CacheMissCount)
+			continue
+		}
+
+		as.NoError(err)
+
+		as.Equal(testCase.expectFsType, fsType)
+		as.Equal(testCase.expectCapacity, capacity)
+		as.Equal(testCase.expectFree, free)
+		as.Equal(testCase.expectAvailable, available)
+		as.Equal(testCase.expectInodes, inodes)
+		as.Equal(testCase.expectInodesFree, inodesFree)
+
+		_, _, _, _, _, _, err = fsCache.FsStats(testCase.device, part)
+		as.Equal(uint32(1), helper.CacheMissCount)
+	}
+}
+
+func TestCacheExpiry(t *testing.T) {
+	as := assert.New(t)
+
+	for _, testCase := range testCases {
+		helper := testStatsHelper{}
+		fsCache := newFsStatsCache(0, &helper)
+		as.Equal(uint32(0), helper.CacheMissCount)
+
+		part := partition{
+			fsType:     testCase.fsType,
+			mountpoint: testCase.mountpoint,
+			blockSize:  testCase.blockSize,
+		}
+
+		fsCache.FsStats(testCase.device, part)
+		as.Equal(uint32(1), helper.CacheMissCount)
+
+		fsCache.FsStats(testCase.device, part)
+		as.Equal(uint32(2), helper.CacheMissCount)
+	}
+}
+
+func TestRace(t *testing.T) {
+	as := assert.New(t)
+
+	helper := testStatsHelper{}
+	fsCache := newFsStatsCache(0, &helper)
+
+	goroutinesPerTestCase := 20
+
+	wg := sync.WaitGroup{}
+	wg.Add(len(testCases) * goroutinesPerTestCase)
+
+	for _, testCase := range testCases {
+		// Copy into local variables to avoid tripping the race detector
+		device := testCase.device
+		part := partition{
+			fsType:     testCase.fsType,
+			mountpoint: testCase.mountpoint,
+			blockSize:  testCase.blockSize,
+		}
+
+		for i := 0; i < goroutinesPerTestCase; i++ {
+			go func() {
+				defer wg.Done()
+
+				fsCache.FsStats(device, part)
+			}()
+		}
+	}
+
+	wg.Wait()
+	as.Equal(uint(len(testCases)*goroutinesPerTestCase), helper.CacheMissCount)
+}
diff --git a/fs/types.go b/fs/types.go
index b345339..f7eeaf1 100644
--- a/fs/types.go
+++ b/fs/types.go
@@ -105,3 +105,8 @@ type PartitionCache interface {
 	DeviceNameForLabel(label string) (string, error)
 	ApplyOverLabels(f func(label string, device string) error) error
 }
+
+type FsStatsCache interface {
+	Clear()
+	FsStats(dev string, part partition) (FsType, uint64, uint64, uint64, uint64, uint64, error)
+}
diff --git a/manager/container.go b/manager/container.go
index b1d8d33..531604b 100644
--- a/manager/container.go
+++ b/manager/container.go
@@ -19,7 +19,6 @@ import (
 	"fmt"
 	"io/ioutil"
 	"math"
-	"math/rand"
 	"os/exec"
 	"path"
 	"regexp"
@@ -35,6 +34,7 @@ import (
 	info "github.com/google/cadvisor/info/v1"
 	"github.com/google/cadvisor/info/v2"
 	"github.com/google/cadvisor/summary"
+	"github.com/google/cadvisor/utils"
 	"github.com/google/cadvisor/utils/cpuload"
 
 	units "github.com/docker/go-units"
@@ -79,17 +79,6 @@ type containerData struct {
 	collectorManager collector.CollectorManager
 }
 
-// jitter returns a time.Duration between duration and duration + maxFactor * duration,
-// to allow clients to avoid converging on periodic behavior.  If maxFactor is 0.0, a
-// suggested default value will be chosen.
-func jitter(duration time.Duration, maxFactor float64) time.Duration {
-	if maxFactor <= 0.0 {
-		maxFactor = 1.0
-	}
-	wait := duration + time.Duration(rand.Float64()*maxFactor*float64(duration))
-	return wait
-}
-
 func (c *containerData) Start() error {
 	go c.housekeeping()
 	return nil
@@ -368,7 +357,7 @@ func (self *containerData) nextHousekeeping(lastHousekeeping time.Time) time.Tim
 		}
 	}
 
-	return lastHousekeeping.Add(jitter(self.housekeepingInterval, 1.0))
+	return lastHousekeeping.Add(utils.Jitter(self.housekeepingInterval, 1.0))
 }
 
 // TODO(vmarmol): Implement stats collecting as a custom collector.
diff --git a/utils/jitter.go b/utils/jitter.go
new file mode 100644
index 0000000..5822655
--- /dev/null
+++ b/utils/jitter.go
@@ -0,0 +1,31 @@
+// Copyright 2015 Google Inc. All Rights Reserved.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"math/rand"
+	"time"
+)
+
+// Jitter returns a time.Duration between duration and duration + maxFactor * duration,
+// to allow clients to avoid converging on periodic behavior.  If maxFactor is 0.0, a
+// suggested default value will be chosen.
+func Jitter(duration time.Duration, maxFactor float64) time.Duration {
+	if maxFactor <= 0.0 {
+		maxFactor = 1.0
+	}
+	wait := duration + time.Duration(rand.Float64()*maxFactor*float64(duration))
+	return wait
+}
-- 
2.7.4

